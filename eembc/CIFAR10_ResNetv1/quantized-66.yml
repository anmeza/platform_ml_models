save_dir: resnet_v1_eembc_quantized_66_acc

model:
  name: resnet_v1_eembc_quantized
  filters: 
  - 8
  - 8
  - 16
  - 16
  - 16
  - 16
  - 2
  - 2
  l1: 0
  l2: 1e-4
  kernels:
  - 3
  - 3
  - 3
  - 3
  - 3
  - 1
  - 3
  - 3
  - 1
  - 3
  - 3
  strides: 
  - '111'
  - '212'
  - '212'
  - '220'
  skip: 0
  avg_pooling: 0

pruning:
  sparsity: 1.0

fit:
  compile:
    initial_lr: 0.001
    lr_decay: 0.99
    optimizer: Adam
    loss: categorical_crossentropy
  epochs: 200
  patience: 40
  batch_size: 32
  verbose: 1

quantization:
  logit_total_bits: 7
  logit_int_bits: 2
  logit_quantizer: quantized_bits
  activation_total_bits: 7
  activation_int_bits: 2
  activation_quantizer: quantized_relu
  alpha: 1
  use_stochastic_rounding: 0

convert:
  OutputDir: my-hls-test-quantized_66_acc
  XilinxPart: xc7z020clg400-1
  Backend: Pynq
  IOType: io_stream
  Precision: ap_fixed<10,7,AP_RND,AP_SAT>
  PrecisionActivation: ap_ufixed<8,3,AP_RND,AP_SAT>
  ReuseFactor: 20000 #16384
  Trace: 0
  Build: 1
  ClockPeriod: 20
  Strategy: Resource
  Override:
    input_1:
      Precision: ap_ufixed<8,1>
    softmax:
      Strategy: Stable
      Precision: ap_ufixed<14,2,AP_RND,AP_SAT>
      exp_table_t: ap_ufixed<18,8,AP_RND,AP_SAT>
      inv_table_t: ap_ufixed<18,8,AP_RND,AP_SAT>
    q_conv2d_batchnorm:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_1:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_2:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_3:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_4:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_5:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_6:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_7:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_conv2d_batchnorm_8:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
    q_dense:
      accum_t: ap_fixed<16,6,AP_RND,AP_SAT>
